{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from DianaModules.utils.BaseModules import DianaModule\n",
    "from DianaModules.models.cifar10.LargeResnet import resnet20\n",
    "import torchvision\n",
    "import torchvision.datasets as ds \n",
    "from DianaModules.utils.BaseModules import DianaModule\n",
    "from DianaModules.utils.serialization.Loader import ModulesLoader\n",
    "from DianaModules.utils.serialization.Serializer import ModulesSerializer\n",
    "from DianaModules.core.Operations import DIANAReLU\n",
    "from pathlib import Path\n",
    "import torch.utils.data as ut\n",
    "output_weights_path = str(Path(\"zoo/cifar10/workshop/resnet20\").absolute())\n",
    "train_dataset =  ds.CIFAR10('./data/cifar10/train', train =True ,download=True, transform=torchvision.transforms.Compose([torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.RandomCrop(32, 4),torchvision.transforms.ToTensor() ,torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]))\n",
    "test_dataset =  ds.CIFAR10('./data/cifar10/validation', train =False,download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))] ) )\n",
    "data_loader = {'train': ut.DataLoader(train_dataset, batch_size=128, shuffle=True, pin_memory=True , num_workers=8) , 'validate' : ut.DataLoader(test_dataset, batch_size=128, shuffle=True  ,pin_memory=True, num_workers=8)}\n",
    "train_scale = torch.Tensor([0.03125]) #found by having train_dataset go thorugh 8-bit quantizer (check datasetscale file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Floating Point Model \n",
    "First step of using the training framework is defining your own PyTorch model like in the example below. You can choose to train or load the model's weights as you usually would.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DianaModules.models.cifar10.LargeResnet import resnet20 \n",
    "\n",
    "custom_model = resnet20() \n",
    "\n",
    "FP_weights = output_weights_path +  \"/FP_weights.pth\"\n",
    "custom_model.load_state_dict(DianaModule.remove_data_parallel(torch.load(FP_weights, map_location='cpu')['state_dict']) )# To load previously trained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion Process \n",
    "## Fake-Quantization \n",
    "Now that you have your floating point PyTorch model defined, we can start with the conversion process. The first step in the conversion process is to fake-quantize the original floating-point model. Each layer has certain characteristics that describe it, i.g. the core choice (analog core or digital core). We allow the user to determine some of these characteristics by editing the yaml file generated by the serialized fake-quantized model. If you don't have a serialized file for the model, then you can run the model serialization step first or just use the conversion with the default behaviour. Notice that there are some constraints that will be clarified in the serialization process in later updates to the training framework.\n",
    "#### Model Description Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_descriptions_pth = str(Path(\"serialized_models/resnet20.yaml\").absolute())\n",
    "\n",
    "loader = ModulesLoader()\n",
    "module_descriptions = loader.load(module_descriptions_pth) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_quantized_model = DianaModule.from_trainedfp_model(model=custom_model , modules_descriptors=module_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " After the conversion we will need to run some functions on the fake-quantized model; for this, we can use the DianaModule class that can be used as a wrapper to the model and use some of the useful functions it implements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mixed_model = DianaModule(fake_quantized_model) #Mixed_model.gmodule = fake_quantized_model\n",
    "Mixed_model.attach_train_dataset(train_dataset, train_scale)\n",
    "Mixed_model.attach_validation_dataset(test_dataset,train_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serializer = ModulesSerializer(Mixed_model.gmodule)  \n",
    "serializer.dump(module_descriptions_pth) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization Initialization\n",
    "By running forward passes and observing the weights and activations, we can initialize our quantization parameters. You can quantize only the linear layers, only the activations(ReLU & Identity) , or both. However, if you quantize both activations and layers and you decide to retrain the model, be careful because the learning rate is usually to high for training the ReLU's clipping parameters (we use PACT), so it's best to freeze the training of ReLU layers and once some of the accuracy is recouped, you unfreeze the ReLU layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mixed_model.initialize_quantization(count=1)# for initialization of both linear layers and activations\n",
    "#Mixed_model.initialize_quantization_no_activation(count=1)# for initialization linear layers\n",
    "#Mixed_model.initialize_quantization_activations(count=1)# for initialization of activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Example\n",
    "You can skip this example and just load some of the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If both linear layers and activation we quantized layers are quantized simultaneously\n",
    "for _, module in Mixed_model.named_modules(): \n",
    "    if isinstance(module, DIANAReLU)  :\n",
    "        module.freeze() \n",
    "FQ_weights = '' #path you want to save your weights on \n",
    "optimizer = torch.optim.SGD(Mixed_model.gmodule.parameters() , lr=0.1 , momentum=0.4) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer= optimizer, mode='max' , factor=0.1, patience=4)\n",
    "params =  DianaModule.train(Mixed_model.gmodule,optimizer,data_loader, epochs=120, model_save_path=FQ_weights , scheduler = scheduler) # training with scale of layer before relu clipped to 2 \n",
    "\n",
    "\n",
    "# If both linear layers and activation we quantized layers are quantized simultaneously\n",
    "for _, module in Mixed_model.named_modules(): \n",
    "    if isinstance(module, DIANAReLU)  :\n",
    "        module.thaw() \n",
    "\n",
    "# retrain again to use the activation-quantization training algorithms like PACT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load the pretrained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FQ_weights_act = output_weights_path + \"/FQ_weights_act.pth\" \n",
    "Mixed_model.gmodule.load_state_dict(DianaModule.remove_data_parallel(torch.load(FQ_weights_act, map_location='cpu')['state_dict']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Mapping \n",
    "Note: While training, you might have moved the model to a different device, but be sure to return the model to the cpu for the conversion step.\n",
    "#### Model Conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mixed_model.map_to_hw()\n",
    "HWmapped_weights = output_weights_path + \"/HWmapped_weights.pth\" \n",
    "Mixed_model.gmodule.load_state_dict(DianaModule.remove_data_parallel(torch.load(HWmapped_weights, map_location='cpu')['state_dict'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Training \n",
    "Same as before, you can train the model as you usually would train a standard PyTorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Integrization\n",
    "Same as before, be sure to return the model to the cpu for the conversion step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mixed_model.integrize_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Export\n",
    "For the final step, we export out integrized model as an ONNX file. Note: You'll get an error for the DORY Annotator, but that's fine you can ignore it. It will be fixed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"backend/cifar10/resnet20\")\n",
    "\n",
    "Mixed_model.gmodule.to('cpu')\n",
    "Mixed_model.export_model(str(data_folder.absolute()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ecb893e122cd98e1f79a856086ed6f6e89b082d9fff7db56f6fafe39767760f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
