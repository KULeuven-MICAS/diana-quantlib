# diana-training-fmw
repository for the training framework of diana

# Workflow
- ## Pytorch Model 
  The first step is to define a pytorch nn module and making sure that it is functional. If possible,  always use pytorch **nn.Module**'s as the building blocks for you model to ensure compatibility with the library. Non-modular functions should be avoided when possible. For example, it would be better to use nn.ReLU instead of nn.functional.relu . 
- ## Iterative Training
  - **Iteration 1**: As regularly done, the torch floating-point Model is trained 
  - **Iteration 2**: Model is fake-quantized to 8-bits and a graph of the model is generated. The model is then re-trained to regain the lost accuracy 
  - **Iteration 3**: Model is re-quantized to diana specific parameters using **map_scale**. For example, the weights of a convolution in the analog core are mapped to terneray. After the mapping, the model is retrained again. 
  - **Iteration 4**: Scales are clipped to the closest power of 2 and model is retrained. 
- ## Graph Generation 
    After training is done, the fake-quantized model is integrized ([check true-quantization section for more info](#true-quantization) 
) and an ONNX model is generated and exported with the DORY-specific annotations 
# DianaModule 
This is the base module of the training framework. 
# Model Conversion
## Structure 
The neural network conversion process in Quantlib, whether it be floating-point to fake-quantized or fake-quantized to true-quantized, is implemented as a list of graph editors that edit the operands and the operations in the computational graph generated by the torch symbolic tracer. Each editor is made out of a finder and an applier. The finder is responsible for collecting application points matching a certain criteria, while the applier is responsible for actually changing the graph or adding meta-data(annotation). 
## Fake-Quantization 
- ### Cannonicalisation
  - Non-modular functions converter (built-in quantlib): Funcitonal torch operations like torch.functional.relu is converted to modular operation (nn.ReLU)
  - Batchnorm Folding: Bias of conv layer is absorbed into batchnorm layer 
- ### Fake quantization 
    - Module Wise Converter (built in)
    - Interposer [Quantizers sim] (extended) 
    - Harmonise adds (extended) 
    - Activaitons fuser (extended)
## True-Quantization 
- Annotation 
- EpsTunnel Inserter 
- Linear operation integrizer (extended) 
- Requantizer (extended) 
- Custom Editors (depending on network)
- EpsConstructSimplifier  
- EpsTunnelRemover 


# Quantlib Framework 


## Important properties 
### **QrangeSpecType**

property that specifies the quantisation integer ranges. The qrangespectype can be defined by a dictionary containing 2 different attributes: the quantized range and offset. The range can be defined using the key words **bitwidth** , **n_levels** , **limpbitwidth**
. The offset can be defined using the key words **offset** and **signed**. Note: the qrangespectype can also be defined directly by assigning it to **'binary'** or **'ternary'**. 
### **QGranularitySpecType**
This property defines how the statistics are collected and how scales are calculated. The granularity can be defined **'per-array'** (per layer) or **'per-outchannel_weights'** (per channel). Notice that quantized activation functions must be implemented with a granularity type of **'per-array'**
### **QHParamsInitStrategySpecType**
This property defines how the quatnization hyperparameters will be initialized after the observation. Notice that at least 1 forward pass must be completed before initialization. The currently implemented values are: **'const'**, **'minmax'**, **'meanstd'**

## Observer Class 
When initialising any quantlib class, an observer object is created. This observer object is responsible for collecting **tensor statistics** about the input, and is initialized with the **QGranularitySpecType** of the class. 
## Quantisation Operation

When defining your custom quantized classes, make sure to register your own autograd quantization function using the _register_qop function and implement it in the _call_op function. 

## Extending Quantlib 
[Check Model Conversion](#model-conversion)
       